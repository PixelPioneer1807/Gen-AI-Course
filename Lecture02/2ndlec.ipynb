{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b752e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4aaae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\" I love AIML\", \"My Name is Shivam Srivastava\",\"I study in Bennett University\", \"My CGPA is 8.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c73a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset into its Numerical Representation \n",
    "\n",
    "# Step --> 1 \n",
    "\n",
    "unique_words=list(set(\" \".join(corpus).split()))  #join the entire and split on \n",
    "#spaces and add them to a set to remove deuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1be6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CGPA',\n",
       " 'My',\n",
       " 'love',\n",
       " 'AIML',\n",
       " 'Shivam',\n",
       " 'University',\n",
       " '8.0',\n",
       " 'Name',\n",
       " 'in',\n",
       " 'Bennett',\n",
       " 'I',\n",
       " 'study',\n",
       " 'Srivastava',\n",
       " 'is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6db94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'CGPA')\n",
      "(1, 'My')\n",
      "(2, 'love')\n",
      "(3, 'AIML')\n",
      "(4, 'Shivam')\n",
      "(5, 'University')\n",
      "(6, '8.0')\n",
      "(7, 'Name')\n",
      "(8, 'in')\n",
      "(9, 'Bennett')\n",
      "(10, 'I')\n",
      "(11, 'study')\n",
      "(12, 'Srivastava')\n",
      "(13, 'is')\n"
     ]
    }
   ],
   "source": [
    "# No we've to assign unique indexes to these values \n",
    "\n",
    "for i in enumerate(unique_words):\n",
    "    print(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0820c637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CGPA': 0,\n",
       " 'My': 1,\n",
       " 'love': 2,\n",
       " 'AIML': 3,\n",
       " 'Shivam': 4,\n",
       " 'University': 5,\n",
       " '8.0': 6,\n",
       " 'Name': 7,\n",
       " 'in': 8,\n",
       " 'Bennett': 9,\n",
       " 'I': 10,\n",
       " 'study': 11,\n",
       " 'Srivastava': 12,\n",
       " 'is': 13}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atore the mapped values in adictionary in form of key value pairs \n",
    "\n",
    "wordToindex={word:i for i,word in enumerate(unique_words)}\n",
    "wordToindex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c5d2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I love AIML\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "10\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3\n",
      "1\n",
      "My Name is Shivam Srivastava\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "7\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "13\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "12\n",
      "1\n",
      "I study in Bennett University\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "10\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "11\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "8\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "9\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "5\n",
      "1\n",
      "My CGPA is 8.0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "13\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# We want one hot encoded data of text\n",
    "\n",
    "one_hot_vector=[]\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    sentence_vector=[]\n",
    "    for word in sentence.split():\n",
    "        vector=[0] * len(unique_words)  ##we've created a sparse vector space\n",
    "        #we've used the length of unique words so that it can be converted into easy numerical features \n",
    "        print(vector)\n",
    "        vector[wordToindex[word]]=1\n",
    "        print(wordToindex[word])\n",
    "        print(vector[wordToindex[word]])\n",
    "        sentence_vector.append(vector)\n",
    "    one_hot_vector.append(sentence_vector)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8450006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector\n",
    "\n",
    "# So far we've convertwd our data into one hot encoding without using any libraries only manual coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method od converting text data to Numerical data \n",
    "# BAG OF WORDS \n",
    "# This is frequency based encoding technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30276afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7689abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(vocabulary=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18503154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1364: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4af48d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f722738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CGPA', 'My', 'love', 'AIML', 'Shivam', 'University', '8.0',\n",
       "       'Name', 'in', 'Bennett', 'I', 'study', 'Srivastava', 'is'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()  #gives all the unique data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47c22147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words is also not able to learn relation of words within sentences \n",
    "# I love NLP is same as NLP love I or anyother permutation of the sentence I love NLP\n",
    "# BOW just optimises the space occupied by sparse matrix in one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccdb1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT ENcoding technique \n",
    "\n",
    "# TF-IDF Term Frequency - inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_tf_idf=TfidfVectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3133f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=vect_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05a74220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.38274272,\n",
       "        0.        , 0.38274272, 0.48546061, 0.48546061, 0.48546061,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.5       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.5       ],\n",
       "       [0.        , 0.        , 0.66767854, 0.        , 0.52640543,\n",
       "        0.        , 0.52640543, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6f0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
